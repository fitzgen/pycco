
Section 0: doc

This module houses the main classes you will interact with,
:class:`.Cluster` and :class:`.Session`.


Section 0: code
from __future__ import absolute_import

from concurrent.futures import ThreadPoolExecutor



Section 1: doc
default to gevent when we are monkey patched, otherwise if libev is available, use that as the
default because it's faster than asyncore

Section 1: code
if 'gevent.monkey' in sys.modules:
    from cassandra.io.geventreactor import GeventConnection as DefaultConnection
else:
    try:
        from cassandra.io.libevreactor import LibevConnection as DefaultConnection  # NOQA
    except ImportError:
        from cassandra.io.asyncorereactor import AsyncoreConnection as DefaultConnection  # NOQA


Section 2: doc
Forces load of utf8 encoding module to avoid deadlock that occurs
if code that is being imported tries to import the module in a seperate
thread.
See http://bugs.python.org/issue10923

Section 2: code
"".encode('utf8')

log = logging.getLogger(__name__)


DEFAULT_MIN_REQUESTS = 5
DEFAULT_MAX_REQUESTS = 100

DEFAULT_MIN_CONNECTIONS_PER_LOCAL_HOST = 2
DEFAULT_MAX_CONNECTIONS_PER_LOCAL_HOST = 8

DEFAULT_MIN_CONNECTIONS_PER_REMOTE_HOST = 1
DEFAULT_MAX_CONNECTIONS_PER_REMOTE_HOST = 2


_NOT_SET = object()



Section 3: doc

Raised when an operation is attempted but all connections are
busy, defunct, closed, or resulted in errors when used.


Section 3: code
class NoHostAvailable(Exception):
Section 4: doc

A map of the form ``{ip: exception}`` which details the particular
Exception that was caught for each host the operation was attempted
against.


Section 4: code

    errors = None
Section 5: doc
''
Section 5: code


Section 6: doc
''
Section 6: code
    def __init__(self, message, errors):
        Exception.__init__(self, message, errors)
        self.errors = errors



Section 7: doc
Helper for run_in_executor()

Section 7: code
def _future_completed(future):
Section 8: doc
''
Section 8: code
    exc = future.exception()
    if exc:
        log.debug("Failed to run task on executor", exc_info=exc)



Section 9: doc

A decorator to run the given method in the ThreadPoolExecutor.


Section 9: code
def run_in_executor(f):
Section 10: doc
''
Section 10: code


Section 11: doc

The main class to use when interacting with a Cassandra cluster.
Typically, one instance of this class will be created for each
separate Cassandra cluster that your application interacts with.

Example usage::

    &gt;&gt;&gt; from cassandra.cluster import Cluster
    &gt;&gt;&gt; cluster = Cluster(['192.168.1.1', '192.168.1.2'])
    &gt;&gt;&gt; session = cluster.connect()
    &gt;&gt;&gt; session.execute(&quot;CREATE KEYSPACE ...&quot;)
    &gt;&gt;&gt; ...
    &gt;&gt;&gt; cluster.shutdown()



Section 11: code
    @wraps(f)
    def new_f(self, *args, **kwargs):

        try:
            future = self.executor.submit(f, self, *args, **kwargs)
            future.add_done_callback(_future_completed)
        except Exception:
            log.exception("Failed to submit task to executor")

    return new_f


class Cluster(object):
Section 12: doc

The server-side port to open connections to. Defaults to 9042.


Section 12: code

    port = 9042
Section 13: doc

If a specific version of CQL should be used, this may be set to that
string version.  Otherwise, the highest CQL version supported by the
server will be automatically used.


Section 13: code

    cql_version = None
Section 14: doc

Whether or not compression should be enabled when possible. Defaults to
:const:`True` and attempts to use snappy compression.


Section 14: code

    compression = True
Section 15: doc

An optional function that accepts one argument, the IP address of a node,
and returns a dict of credentials for that node.


Section 15: code

    auth_provider = None
Section 16: doc

An instance of :class:`.policies.LoadBalancingPolicy` or
one of its subclasses.  Defaults to :class:`~.RoundRobinPolicy`.


Section 16: code

    load_balancing_policy = None
Section 17: doc

An instance of :class:`.policies.ReconnectionPolicy`. Defaults to an instance
of :class:`.ExponentialReconnectionPolicy` with a base delay of one second and
a max delay of ten minutes.


Section 17: code

    reconnection_policy = ExponentialReconnectionPolicy(1.0, 600.0)
Section 18: doc

A default :class:`.policies.RetryPolicy` instance to use for all
:class:`.Statement` objects which do not have a :attr:`~.Statement.retry_policy`
explicitly set.


Section 18: code

    default_retry_policy = RetryPolicy()
Section 19: doc

A factory function which creates instances of
:class:`.policies.ConvictionPolicy`.  Defaults to
:class:`.policies.SimpleConvictionPolicy`.


Section 19: code

    conviction_policy_factory = SimpleConvictionPolicy
Section 20: doc

Whether or not metric collection is enabled.  If enabled, :attr:`.metrics`
will be an instance of :class:`.metrics.Metrics`.


Section 20: code

    metrics_enabled = False
Section 21: doc

An instance of :class:`.metrics.Metrics` if :attr:`.metrics_enabled` is
:const:`True`, else :const:`None`.


Section 21: code

    metrics = None
Section 22: doc

A optional dict which will be used as kwargs for ``ssl.wrap_socket()``
when new sockets are created.  This should be used when client encryption
is enabled in Cassandra.

By default, a ``ca_certs`` value should be supplied (the value should be
a string pointing to the location of the CA certs file), and you probably
want to specify ``ssl_version`` as ``ssl.PROTOCOL_TLSv1`` to match
Cassandra's default protocol.


Section 22: code

    ssl_options = None
Section 23: doc

An optional list of tuples which will be used as arguments to
``socket.setsockopt()`` for all created sockets.


Section 23: code

    sockopts = None
Section 24: doc

The maximum duration (in seconds) that the driver will wait for schema
agreement across the cluster. Defaults to ten seconds.


Section 24: code

    max_schema_agreement_wait = 10
Section 25: doc

An instance of :class:`cassandra.metadata.Metadata`.


Section 25: code

    metadata = None
Section 26: doc

This determines what event loop system will be used for managing
I/O with Cassandra.  These are the current options:

* :class:`cassandra.io.asyncorereactor.AsyncoreConnection`
* :class:`cassandra.io.libevreactor.LibevConnection`

By default, ``AsyncoreConnection`` will be used, which uses
the ``asyncore`` module in the Python standard library.  The
performance is slightly worse than with ``libev``, but it is
supported on a wider range of systems.

If ``libev`` is installed, ``LibevConnection`` will be used instead.


Section 26: code

    connection_class = DefaultConnection
Section 27: doc

A timeout, in seconds, for queries made by the control connection, such
as querying the current schema and information about nodes in the cluster.
If set to :const:`None`, there will be no timeout for these queries.


Section 27: code

    control_connection_timeout = 2.0
Section 28: doc
''
Section 28: code

    sessions = None
    control_connection = None
    scheduler = None
    executor = None
    _is_shutdown = False
    _is_setup = False
    _prepared_statements = None
    _prepared_statement_lock = Lock()

    _listeners = None
    _listener_lock = None


Section 29: doc

Any of the mutable Cluster attributes may be set as keyword arguments
to the constructor.


Section 29: code
    def __init__(self,
                 contact_points=("127.0.0.1",),
                 port=9042,
                 compression=True,
                 auth_provider=None,
                 load_balancing_policy=None,
                 reconnection_policy=None,
                 default_retry_policy=None,
                 conviction_policy_factory=None,
                 metrics_enabled=False,
                 connection_class=None,
                 ssl_options=None,
                 sockopts=None,
                 cql_version=None,
                 executor_threads=2,
                 max_schema_agreement_wait=10,
                 control_connection_timeout=2.0):
Section 30: doc
''
Section 30: code
        self.contact_points = contact_points
        self.port = port
        self.compression = compression


Section 31: doc
let Session objects be GC'ed (and shutdown) when the user no longer
holds a reference. Normally the cycle detector would handle this,
but implementing __del__ prevents that.

Section 31: code
        self.sessions = WeakSet()
        self.metadata = Metadata(self)
        self.control_connection = None
        self._prepared_statements = WeakValueDictionary()


Section 32: doc

Gets the minimum number of connections per Session that will be opened
for each host with :class:`~.HostDistance` equal to `host_distance`.
The default is 2 for :attr:`~HostDistance.LOCAL` and 1 for
:attr:`~HostDistance.REMOTE`.


Section 32: code
    def get_core_connections_per_host(self, host_distance):
Section 33: doc
''
Section 33: code
        return self._core_connections_per_host[host_distance]


Section 34: doc
''
Section 34: code
    def __del__(self):

Section 35: doc
we don't use shutdown() because we want to avoid shutting down
Sessions while they are still being used (in case there are no
longer any references to this Cluster object, but there are
still references to the Session object)

Section 35: code
        if not self._is_shutdown:
            if self.scheduler:
                self.scheduler.shutdown()
            if self.control_connection:
                self.control_connection.shutdown()
            if self.executor:
                self.executor.shutdown(wait=False)


Section 36: doc
''
Section 36: code
    def _on_up_future_completed(self, host, futures, results, lock, finished_future):
        with lock:
            futures.discard(finished_future)

            try:
                results.append(finished_future.result())
            except Exception as exc:
                results.append(exc)

            if futures:
                return

        try:

Section 37: doc
all futures have completed at this point

Section 37: code
            for exc in [f for f in results if isinstance(f, Exception)]:
                log.error("Unexpected failure while marking node %s up:", host, exc_info=exc)
                self._cleanup_failed_on_up_handling(host)
                return

            if not all(results):
                log.debug("Connection pool could not be created, not marking node %s up", host)
                self._cleanup_failed_on_up_handling(host)
                return


Section 38: doc
mark the host as up and notify all listeners

Section 38: code
            host.set_up()
            for listener in self.listeners:
                listener.on_up(host)
        finally:
            host._handle_node_up_condition.acquire()
            if host._currently_handling_node_up:
                host._currently_handling_node_up = False
                host._handle_node_up_condition.notify()
            host._handle_node_up_condition.release()


Section 39: doc
see if there are any pools to add or remove now that the host is marked up

Section 39: code
        for session in self.sessions:
            session.update_created_pools()



